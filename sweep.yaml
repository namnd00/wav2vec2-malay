program: train.py
project: wav2vec4malay
method: bayes
metric:
  name: wer
  goal: minimize
parameters:
  attention_dropout:
    values:
      - 0.1
#    distribution: log_uniform
#    # from 0.1/5 to 0.1*5 - values provided are ln(min) -> ln(max)
#    min: -3.9
#    max: -0.7
  activation_dropout:
    values:
      - 0.1
#    distribution: log_uniform
#    min: -3.9
#    max: -0.7
  hidden_dropout:
    values:
      - 0.1
#    distribution: log_uniform
#    min: -3.9
#    max: -0.7
  feat_proj_dropout:
    values:
      - 0.0
#    distribution: log_uniform
#    min: -3.9
#    max: -0.7
  mask_time_prob:
    values:
      - 0.05
#    distribution: log_uniform
#    min: -3.9
#    max: -0.7
  layerdrop:
    values:
      - 0.1
#    distribution: log_uniform
#    # from 0.05/2 to 0.05*2 - values provided are ln(min) -> ln(max)
#    min: -4.6
#    max: -1.6
  learning_rate:
    values:
      - 5e-5
#    distribution: log_uniform
#    min: -9.2
#    max: -6.9
  weight_decay:
    values:
      - 0.005
  warmup_steps:
    values:
      - 1000
  gradient_accumulation_steps:
    values:
      - 1
  lr_scheduler_type:
    values:
      - cosine_with_restarts
command:
  - python
  - ${program}
  - "--model_name_or_path"
  - "facebook/wav2vec2-large-xlsr-53"
  - "--dataset_config_name"
  - "./datasets"
  - "--train_split_name"
  - "./datasets/submission.csv"
  - "--output_dir"
  - "./model"
  - "--overwrite_output_dir"
  - "--num_train_epochs"
  - 20
  - "--per_device_train_batch_size"
  - 16
  - "--per_device_eval_batch_size"
  - 16
  - "--fp16"
  - "--freeze_feature_extractor"
  - "--group_by_length"
  - "--gradient_checkpointing"
  - "--do_train"
  - "--do_eval"
  - "--save_total_limit"
  - 1
  - "--save_strategy"
  - "epoch"
  - "--evaluation_strategy"
  - "epoch"
  - ${args}